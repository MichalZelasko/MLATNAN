{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sudo apt install curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mzelasko/miniconda3/envs/ML4Neuro/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.io.image import load_nifti_data, load_nifti, save_nifti\n",
    "from dipy.direction import peaks\n",
    "from dipy.reconst import shm\n",
    "from dipy.tracking import utils\n",
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "from dipy.tracking.stopping_criterion import BinaryStoppingCriterion\n",
    "from dipy.tracking.streamline import Streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.io.stateful_tractogram import Space, StatefulTractogram\n",
    "from dipy.io.streamline import save_trk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_import_text(number = 10159, short=False):\n",
    "    # choose lines containing given number of directory\n",
    "    f = open(\"./Data/ds000030-1.0.0.sh\", \"r\")\n",
    "    lines_array = f.readlines()\n",
    "    lines_import_text = \"\"\n",
    "    nii_gz_filenames = []\n",
    "    for line in lines_array:\n",
    "        if f\"sub-{number}\" in line[-60:] and (not short or \"dwi\" in line): \n",
    "            lines_import_text += line\n",
    "            if \"nii.gz\" in line[-10:]:\n",
    "                words = line.split(\" \")\n",
    "                nii_gz_filenames.append(words[-1][:-1])\n",
    "    return lines_import_text, nii_gz_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bash_script(number = 10159, do_import = True, short=False):\n",
    "    # removing previous script\n",
    "    try:\n",
    "        os.system('rm ./Data/import_process_script.sh')\n",
    "    except:\n",
    "        print(\"No script file to remove!\")\n",
    "\n",
    "    # cut part of import script to download one directory\n",
    "    bin_bash = f\"#!/bin/bash\\n\"\n",
    "    import_data, nii_gz_filenames = take_import_text(number, short)\n",
    "    if not do_import: import_data = \"\"\n",
    "    \n",
    "    # extracting mask file from nii.gz files\n",
    "    convert_data_fsl = \"export FSLOUTPUTTYPE=NIFTI_GZ\\nexport PATH=\\\"/usr/local/fsl/bin$PATH\\\"\\nexport FSLDIR=\\\"/usr/local/fsl\\\"\\n\"\n",
    "    for filename in nii_gz_filenames:\n",
    "        convert_data_fsl += f\"bet ./{filename} ./{filename[:-7]}mask{filename[-7:]}\\n\" #f\"dir=\\\"./Data/sub-{number}\\\"\\nnifti_dir=\\\"processed-data/sub-{number}/\\\"\\nmkdir processed-data\\nmkdir $nifti_dir\\ndcm2niix -o $nifti_dir $dir\"\n",
    "        # convert_data_fsl += f\"fast -n 4 ./{filename}\" #-o ./{filename[:-7]}mask{filename[-7:]}\\n\n",
    "    \n",
    "    # create script and write it to file\n",
    "    script_text = bin_bash + import_data + convert_data_fsl\n",
    "    # print(script_text)\n",
    "    f = open(\"./Data/import_process_script.sh\", \"w\")\n",
    "    f.write(script_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# print(\"export FSLOUTPUTTYPE=NIFTI_GZ\\nexport PATH=\\\"/usr/local/fsl/bin$PATH\\\"\\nexport FSLDIR=\\\"/usr/local/fsl\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of function creating singlw directory downloading script\n",
    "# create_bash_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_script(number = 10159, do_import = True, short=False):\n",
    "    # create script importing and converting single directory\n",
    "    create_bash_script(number=number, do_import=do_import, short=short)\n",
    "\n",
    "    # run script in bash\n",
    "    os.system('bash ./Data/import_process_script.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of running script for downloading files\n",
    "# and creating brain masks necessary for further analyse\n",
    "# run_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_to_streamlines(filename = None, number = 10159, do_draw = False):\n",
    "    # load data\n",
    "    if filename == None:\n",
    "        data, affine, load_img = load_nifti(f\"./sub-{number}/dwi/sub-{number}_dwi.nii.gz\", return_img=True) #f\"./sub-{number}/anat/sub-10159_T1w.nii.gz\"\n",
    "        bvals, bvecs = read_bvals_bvecs(f\"./sub-{number}/dwi/sub-{number}_dwi.bval\", f\"./sub-{number}/dwi/sub-{number}_dwi.bvec\")\n",
    "        gtab = gradient_table(bvals, bvecs)\n",
    "        labels = load_nifti_data(f\"./sub-{number}/dwi/sub-{number}_dwimask.nii.gz\")\n",
    "        # labels = load_nifti_data(f\"./sub-{number}/dwi/sub-{number}_dwi_seg.nii.gz\")\n",
    "    else:\n",
    "        data, affine, load_img = load_nifti(filename, return_img=True) #f\"./sub-{number}/anat/sub-10159_T1w.nii.gz\"\n",
    "        bvals, bvecs = read_bvals_bvecs(f\"./sub-{number}/dwi/sub-{number}_dwi.bval\", f\"./sub-{number}/dwi/sub-{number}_dwi.bvec\")\n",
    "        gtab = gradient_table(bvals, bvecs)\n",
    "        labels = load_nifti_data(filename[:-7] + \"mask.nii.gz\")\n",
    "        # labels = load_nifti_data(f\"./sub-{number}/dwi/sub-{number}_dwi_seg.nii.gz\")\n",
    "\n",
    "    # choosing part of the brain\n",
    "    # white_matter = binary_dilation((labels == 1) | (labels == 2)) \n",
    "    # it probably doesn't work this way, white_matter mask may look different in this files\n",
    "    try:\n",
    "        # print(labels.shape, labels)\n",
    "        white_matter = binary_dilation((labels == 1) | (labels == 2)) \n",
    "        csamodel = shm.CsaOdfModel(gtab, 6)\n",
    "        # print(white_matter.shape, data.shape)\n",
    "        csapeaks = peaks.peaks_from_model(model=csamodel,\n",
    "                                        data=data,\n",
    "                                        sphere=peaks.default_sphere,\n",
    "                                        relative_peak_threshold=.8,\n",
    "                                        min_separation_angle=45)\n",
    "\n",
    "        affine = np.eye(4)\n",
    "        seeds = utils.seeds_from_mask(white_matter, affine, density=1)\n",
    "        stopping_criterion = BinaryStoppingCriterion(white_matter)\n",
    "\n",
    "        # choosing tracks model\n",
    "        streamline_generator = LocalTracking(csapeaks, stopping_criterion, seeds,\n",
    "                                            affine=affine, step_size=0.5) # we can change stopping criterion here\n",
    "        streamlines = Streamlines(streamline_generator)\n",
    "\n",
    "        # choosing proper brain slice (verify if it is necessary)\n",
    "        cc_slice = labels == 2\n",
    "        cc_streamlines = utils.target(streamlines, affine, cc_slice)\n",
    "        cc_streamlines = Streamlines(cc_streamlines)\n",
    "\n",
    "        # save streamlines\n",
    "        sft = StatefulTractogram(cc_streamlines, load_img, Space.VOX)\n",
    "        save_trk(sft, \"cc_streamlines.trk\")\n",
    "\n",
    "        # create connectivity matrix\n",
    "        # print(cc_streamlines)\n",
    "        M, _ = utils.connectivity_matrix(cc_streamlines, affine,\n",
    "                                            labels.astype(np.uint8),\n",
    "                                            return_mapping=True,\n",
    "                                            mapping_as_streamlines=True)\n",
    "        M[:3, :] = 0\n",
    "        M[:, :3] = 0\n",
    "\n",
    "        # plot cennectivity matrix\n",
    "        if do_draw:\n",
    "            plt.imshow(np.log1p(M), interpolation='nearest')\n",
    "            plt.savefig(f\"connectivity-{number}.png\")\n",
    "    except:\n",
    "        print(f\"File = {filename}, number = {number}: Error while processing data\")\n",
    "        M = None\n",
    "\n",
    "    return M   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of processing to streamline\n",
    "# print(process_to_streamlines(number = 10159, do_draw = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conectivity_matrix_to_file(M, number = 10159):\n",
    "    # save connectivity matrix as a .txt file\n",
    "    # filename consists a directory number\n",
    "    f = open(f\"./cc_matrices/connectivity-matrix-{number}.txt\", \"w\")\n",
    "    for row in M:\n",
    "        for cell in row:\n",
    "            f.write(str(cell) + \",\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sub_directory(number):\n",
    "    # remove downloaded directory with data\n",
    "    try:\n",
    "        os.system(f\"rm -r sub-{number}\")\n",
    "    except:\n",
    "        print(\"No directory to remove!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_record(number = 10159, do_draw = False, short = False):\n",
    "    # main function for data preprocessing\n",
    "    run_script(number, short=short)\n",
    "    M = process_to_streamlines(number = number, do_draw = do_draw)\n",
    "    if M is not None:\n",
    "        return False\n",
    "    else:\n",
    "        write_conectivity_matrix_to_file(M, number=number)\n",
    "        remove_sub_directory(number)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of processing single directory\n",
    "# preprocess_single_record(number = 10159, do_draw = True, short = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_directory_numbers(index = 0):\n",
    "    index = index % 4\n",
    "    f = open(\"./Data/ds000030-1.0.0.sh\", \"r\")\n",
    "    lines_array = f.readlines()\n",
    "    directory_numbers_set = set()\n",
    "    for line in lines_array:\n",
    "        if f\"_dwi.nii.gz\" in line[-60:]: \n",
    "            directory_numbers_set.add(int(line[-17:-12]))\n",
    "    directory_numbers_list = list(directory_numbers_set)\n",
    "    directory_numbers_list = sorted(directory_numbers_list)\n",
    "    n = len(directory_numbers_list)\n",
    "    return directory_numbers_list[index * n // 4:(index + 1) * n // 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10159, 10171, 10189, 10193, 10206, 10217, 10225, 10227, 10228, 10235, 10249, 10269, 10271, 10273, 10274, 10280, 10290, 10292, 10304, 10316, 10321, 10325, 10329, 10339, 10340, 10345, 10347, 10356, 10361, 10365, 10376, 10377, 10388, 10429, 10438, 10440, 10448, 10455, 10460, 10471, 10478, 10487, 10492, 10506, 10517, 10523, 10524, 10525, 10527, 10530, 10557, 10565, 10570, 10575, 10624, 10629, 10631, 10638, 10674, 10678, 10680, 10686, 10692, 10696, 10697] 65\n",
      "[10704, 10707, 10708, 10719, 10724, 10746, 10762, 10779, 10785, 10788, 10844, 10855, 10871, 10877, 10882, 10891, 10893, 10912, 10934, 10940, 10948, 10949, 10958, 10963, 10968, 10975, 10977, 10987, 10998, 11019, 11030, 11044, 11050, 11052, 11059, 11061, 11062, 11066, 11067, 11068, 11077, 11082, 11088, 11090, 11097, 11098, 11104, 11105, 11106, 11108, 11112, 11122, 11128, 11131, 11142, 11143, 11149, 11156, 50004, 50005, 50006, 50007, 50008, 50010, 50013, 50014] 66\n",
      "[50015, 50016, 50020, 50021, 50022, 50023, 50025, 50027, 50029, 50032, 50033, 50034, 50035, 50036, 50038, 50043, 50047, 50048, 50049, 50050, 50051, 50052, 50053, 50054, 50055, 50056, 50058, 50059, 50060, 50061, 50064, 50066, 50067, 50069, 50075, 50076, 50077, 50080, 50081, 50083, 50085, 60001, 60005, 60006, 60008, 60010, 60011, 60012, 60014, 60015, 60016, 60017, 60020, 60021, 60022, 60028, 60030, 60033, 60036, 60037, 60038, 60042, 60043, 60045, 60046] 65\n",
      "[60048, 60049, 60051, 60052, 60053, 60055, 60056, 60057, 60060, 60062, 60065, 60066, 60068, 60070, 60072, 60073, 60074, 60076, 60077, 60078, 60079, 60080, 60084, 60087, 60089, 70001, 70002, 70004, 70007, 70010, 70015, 70017, 70020, 70021, 70022, 70026, 70029, 70033, 70034, 70037, 70040, 70046, 70048, 70049, 70051, 70052, 70055, 70057, 70058, 70060, 70061, 70065, 70068, 70069, 70070, 70072, 70073, 70074, 70075, 70076, 70077, 70079, 70080, 70081, 70083, 70086] 66\n"
     ]
    }
   ],
   "source": [
    "print(choose_directory_numbers(0), len(choose_directory_numbers(0)))\n",
    "print(choose_directory_numbers(1), len(choose_directory_numbers(1)))\n",
    "print(choose_directory_numbers(2), len(choose_directory_numbers(2)))\n",
    "print(choose_directory_numbers(3), len(choose_directory_numbers(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_process_dict(process_dict):\n",
    "    print(\"Dir. num.: | Processed? \")\n",
    "    for key, value in process_dict.items():\n",
    "        print(f\"{str(key).ljust(10)}| {str(value).ljust(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_record_batch(index = 0, short = False):\n",
    "    # download, create brain masks, process data\n",
    "    # from given list of directories (represeted with numbers)\n",
    "    # argument index id personal id for every team member \n",
    "    # running this code in order to preprocess a batch of data\n",
    "    index_list = choose_directory_numbers(index)\n",
    "    process_dict = {}\n",
    "    index_list = [10159, 10171, 50015, 60048, 70001]\n",
    "    for number in index_list:\n",
    "        print(f\"Directory: sub-{number}\")\n",
    "        process_dict[number] = preprocess_single_record(number = number, do_draw = False, short=short)\n",
    "    print_process_dict(process_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: sub-10159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   322  100   322    0     0    245      0  0:00:01  0:00:01 --:--:--   245\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1814  100  1814    0     0    739      0  0:00:02  0:00:02 --:--:--   739\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2007  100  2007    0     0   1554      0  0:00:01  0:00:01 --:--:--  1554\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 30.5M  100 30.5M    0     0   292k      0  0:01:46  0:01:46 --:--:--  786k\n",
      "Warning: An input intended to be a single 3D volume has multiple timepoints. Input will be truncated to first volume, but this functionality is deprecated and will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: sub-10171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   322  100   322    0     0     81      0  0:00:03  0:00:03 --:--:--    81\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1814  100  1814    0     0    592      0  0:00:03  0:00:03 --:--:--   592\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1999  100  1999    0     0   1622      0  0:00:01  0:00:01 --:--:--  1623\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 32.2M  100 32.2M    0     0   295k      0  0:01:51  0:01:51 --:--:--  830k\n",
      "Warning: An input intended to be a single 3D volume has multiple timepoints. Input will be truncated to first volume, but this functionality is deprecated and will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: sub-50015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   322  100   322    0     0    278      0  0:00:01  0:00:01 --:--:--   278\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1814  100  1814    0     0   1708      0  0:00:01  0:00:01 --:--:--  1709\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2168  100  2168    0     0   2214      0 --:--:-- --:--:-- --:--:--  2214\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 38.6M  100 38.6M    0     0   494k      0  0:01:20  0:01:20 --:--:--  561k\n",
      "Warning: An input intended to be a single 3D volume has multiple timepoints. Input will be truncated to first volume, but this functionality is deprecated and will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "# put your index\n",
    "proprocess_record_batch(index = 0, short = True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4Neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
